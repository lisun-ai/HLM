{
  "output_dir": "./results/4-12-1_pretrain/",
  "datasets": "./data/sample_pretrain_data_processed.csv",
  "char2int_dict": "./data/char2int_dict.pkl",
  "int2char_dict": "./data/int2char_dict.pkl",
  "resume_from_checkpoint": "",
  "ignore_data_skip": true,
  "masking_type": "rand_char",
  "aggregation_method": "word_cls",
  "fp16": true,
  "per_device_train_batch_size": 2,
  "num_train_epochs": 3,
  "max_steps": 4000000,
  "gradient_accumulation_steps": 2,
  "warmup_steps": 10000,
  "weight_decay": 0.01,
  "learning_rate": 0.00005,
  "hidden_size": 768,
  "attention_heads": 12,
  "transformer_ff_size": 3072,
  "local_transformer_ff_size": 1536,
  "dropout": 0.1,
  "activation": "gelu",
  "max_codepoint": 1024,
  "max_char_length": 2048,
  "max_char_per_word": 20,
  "max_num_word": 1024,
  "for_token_classification": false,
  "n_local_layer_first": 4,
  "n_global_layer": 12,
  "n_local_layer_last": 1,
  "remove_unused_columns": false,
  "group_by_length": false,
  "dataloader_num_workers": 4,
  "do_train": true,
  "do_eval": false,
  "logging_steps": 1000,
  "save_total_limit": 20,
  "save_steps": 2000,
  "use_token_type": false,
  "use_projection": true,
  "word_context": 1,
  "relative_attention": true,
  "report_to": "wandb"
}
